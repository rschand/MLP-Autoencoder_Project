{"cells":[{"cell_type":"markdown","id":"RErzeF8QJAEY","metadata":{"id":"RErzeF8QJAEY"},"source":["# **Part 0**"]},{"cell_type":"markdown","source":["https://archive.ics.uci.edu/ml/machine-learning-databases/00464/superconduct.zip"],"metadata":{"id":"tPVS9l9Xf1-P"},"id":"tPVS9l9Xf1-P"},{"cell_type":"code","execution_count":null,"id":"1c4a5bdc","metadata":{"id":"1c4a5bdc"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.metrics import log_loss\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","data = pd.read_csv(\"train.csv\")\n","\n","data = data.dropna()\n","\n","quantiles = np.quantile(data[\"critical_temp\"], np.arange(0, 1.1, 0.1))\n","\n","e1 = data[data[\"critical_temp\"] < 3.32]\n","e2 = data[(data[\"critical_temp\"] > 3.32) & (data[\"critical_temp\"] < 8.4)]\n","e3 = data[(data[\"critical_temp\"] > 8.4) & (data[\"critical_temp\"] < 20)]\n","e4 = data[(data[\"critical_temp\"] > 20) & (data[\"critical_temp\"] < 40.56667)]\n","e5 = data[(data[\"critical_temp\"] > 40.56667) & (data[\"critical_temp\"] < 79)]\n","e6 = data[data[\"critical_temp\"] > 79]\n","\n","column_names = pd.DataFrame(data.columns)\n","\n","data.head()\n"]},{"cell_type":"code","execution_count":null,"id":"bdf5181c","metadata":{"id":"bdf5181c"},"outputs":[],"source":["# Add class labels, drop temperature\n","e1[\"label\"] = \"CL1\"\n","e1 = e1.drop(\"critical_temp\", axis=1)\n","\n","e2[\"label\"] = \"CL2\"\n","e2 = e2.drop(\"critical_temp\", axis=1)\n","\n","e3[\"label\"] = \"CL3\"\n","e3 = e3.drop(\"critical_temp\", axis=1)\n","\n","e4[\"label\"] = \"CL4\"\n","e4 = e4.drop(\"critical_temp\", axis=1)\n","\n","e5[\"label\"] = \"CL5\"\n","e5 = e5.drop(\"critical_temp\", axis=1)\n","\n","e6[\"label\"] = \"CL6\"\n","e6 = e6.drop(\"critical_temp\", axis=1)\n","\n","e1.head()\n"]},{"cell_type":"markdown","id":"J83M9xTZJWCZ","metadata":{"id":"J83M9xTZJWCZ"},"source":["# **Part 1**"]},{"cell_type":"code","execution_count":null,"id":"aaefd813","metadata":{"id":"aaefd813"},"outputs":[],"source":["#Q1: We dont need to use SMOTE and one-hot encoding\n","# Randomly select 80% of cases from each class to create the training set\n","train_e1, test_e1 = train_test_split(e1, test_size=0.2, random_state=42)\n","train_e2, test_e2 = train_test_split(e2, test_size=0.2, random_state=42)\n","train_e3, test_e3 = train_test_split(e3, test_size=0.2, random_state=42)\n","train_e4, test_e4 = train_test_split(e4, test_size=0.2, random_state=42)\n","train_e5, test_e5 = train_test_split(e5, test_size=0.2, random_state=42)\n","train_e6, test_e6 = train_test_split(e6, test_size=0.2, random_state=42)\n","\n","# Concatenate the training and test sets for each class to create the final TRAIN and TEST sets\n","train_data = pd.concat([train_e1, train_e2, train_e3, train_e4, train_e5, train_e6])\n","test_data = pd.concat([test_e1, test_e2, test_e3, test_e4, test_e5, test_e6])\n"]},{"cell_type":"code","execution_count":null,"id":"7d50ba62","metadata":{"id":"7d50ba62"},"outputs":[],"source":["# One-hot encoding\n","# train data\n","one_hot_train = pd.get_dummies(train_data[\"label\"])\n","train_data = train_data.drop(\"label\", axis=1)\n","train_data = pd.concat([train_data, one_hot_train], axis=1)\n","\n","# test data\n","one_hot_test = pd.get_dummies(test_data[\"label\"])\n","test_data = test_data.drop(\"label\", axis=1)\n","test_data = pd.concat([test_data, one_hot_test], axis=1)\n","\n","train_data.head()\n"]},{"cell_type":"markdown","id":"OT33y1lXhPfG","metadata":{"id":"OT33y1lXhPfG"},"source":["# **Part 2**"]},{"cell_type":"code","execution_count":null,"id":"855fba55","metadata":{"id":"855fba55"},"outputs":[],"source":["#Q2:\n","# Split the dataset into features and target variable\n","en_col = [\"CL1\", \"CL2\", \"CL3\", \"CL4\", \"CL5\", \"CL6\"]\n","\n","x_train = train_data.drop(en_col, axis=1)\n","y_train = train_data[en_col]\n","x_test = test_data.drop(en_col, axis=1)\n","y_test = test_data[en_col]\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","x_train = pd.DataFrame(scaler.fit_transform(x_train))\n","x_test = pd.DataFrame(scaler.transform(x_test))\n","\n","x_train.head()\n"]},{"cell_type":"code","execution_count":null,"id":"f056e0b5","metadata":{"id":"f056e0b5","scrolled":true},"outputs":[],"source":["from tensorflow.keras.initializers import RandomUniform\n","import time\n","\n","# Validation Split\n","x_train1, x_valid, y_train1, y_valid = train_test_split(x_train, y_train, test_size=0.1)\n","\n","# Define the model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(41, activation='relu', input_shape=(x_train.shape[1],), kernel_initializer=RandomUniform),\n","    tf.keras.layers.Dense(y_train.shape[1], activation='softmax')\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","start_time = time.time()\n","\n","history = model.fit(x_train1, y_train1, batch_size=152, epochs=175, verbose=0, validation_data=(x_valid,y_valid))\n","\n","end_time = time.time()\n","\n","time_per_epoch = (end_time - start_time) / len(history.history['loss'])\n","\n","# Calculate average cross-entropy loss for train and test sets\n","train_preds = model.predict(x_train)\n","train_avCRE = log_loss(y_train, train_preds) / len(x_train)\n","\n","test_preds = model.predict(x_test)\n","test_avCRE = log_loss(y_test, test_preds) / len(x_test)\n","\n","# Extract the training and validation losses for each epoch\n","train_losses_per_epoch = history.history['loss']\n","test_losses_per_epoch = history.history['val_loss']\n","\n","# Calculate the average cross-entropy loss per epoch for train and test sets\n","train_avCRE_per_epoch = [loss / len(x_train) for loss in train_losses_per_epoch]\n","test_avCRE_per_epoch = [loss / len(x_test) for loss in test_losses_per_epoch]\n","\n","print(\"Average Cross Entropy Loss for Train Set:\", train_avCRE)\n","print(\"Average Cross Entropy Loss for Test Set:\", test_avCRE)\n","\n","print(\"Average Cross Entropy Loss per Epoch for Train Set:\", train_avCRE_per_epoch)\n","print(\"Average Cross Entropy Loss per Epoch for Test Set:\", test_avCRE_per_epoch)\n","\n","print(\"Computing time per epoch: \", time_per_epoch)\n"]},{"cell_type":"code","execution_count":null,"id":"8tfqlHxhqZjb","metadata":{"id":"8tfqlHxhqZjb"},"outputs":[],"source":["# 150 epochs was originally used, but the accuracy of the training set was still\n","# continuing to decrease, so I bumped it up to 200\n","# Computation time for 150 epochs: ~3.5 min\n","# Computation time for 200 epochs: ~4.5 min\n","# Computation time for 250 epochs: ~5.5 min\n","# We will use 150 epochs going forward. Test accuracy levels off at around this point.\n","# 200 and 250 epochs were computed to make sure no accuracy gains were left on the table.\n","\n","import matplotlib.pyplot as plt\n","\n","# Plot the loss per epoch\n","plt.plot(train_losses_per_epoch, label='Train')\n","plt.plot(test_losses_per_epoch, label='Test')\n","plt.title('Cross-Entropy Loss per Epoch, h=41')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","# Evaluate the model on the test set\n","test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n","\n","print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))"]},{"cell_type":"code","execution_count":null,"id":"75e02756","metadata":{"id":"75e02756"},"outputs":[],"source":["round(x_train1.shape[0]/100, 0)"]},{"cell_type":"code","execution_count":null,"id":"xkyAMwdIuKEa","metadata":{"id":"xkyAMwdIuKEa"},"outputs":[],"source":["# Create and print the confusion matrix.\n","\n","from sklearn.metrics import confusion_matrix\n","\n","test_pred_labels = np.argmax(test_preds, axis=1)\n","y_test_array = y_test.to_numpy()\n","y_test_labels = np.argmax(y_test_array, axis=1)\n","\n","# Compute the confusion matrix\n","conf_matrix = confusion_matrix(test_pred_labels,y_test_labels)\n","\n","# Normalize the confusion matrix\n","normalized_conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n","\n","# Convert to percentage\n","percentage_conf_matrix = normalized_conf_matrix * 100\n","\n","print(\"Confusion Matrix (Percentage):\")\n","print(\"Confusion Matrix (Percentage):\")\n","for row in percentage_conf_matrix:\n","    formatted_row = [\"{:.2f}\".format(x) for x in row]\n","    print(\" \".join(formatted_row))\n"]},{"cell_type":"markdown","id":"vX3J63TkhFWi","metadata":{"id":"vX3J63TkhFWi"},"source":["# **Part 3**"]},{"cell_type":"code","execution_count":null,"id":"7TvNBOtpD8ez","metadata":{"id":"7TvNBOtpD8ez"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# Create a PCA object with 90% variance explained\n","pca = PCA(n_components=0.9)\n","\n","# Fit the PCA model on the training data\n","pca.fit(x_train)\n","\n","# Transform the training data into the new feature space\n","x_train_pca = pca.transform(x_train)\n","x_test_pca = pca.transform(x_test)\n","\n","x_train_pca = pd.DataFrame(x_train_pca)\n","x_test_pca = pd.DataFrame(x_test_pca)\n","\n","# Print the shape of the transformed data\n","print(\"Shape of transformed data:\", x_train_pca.shape)\n"]},{"cell_type":"code","execution_count":null,"id":"Ur7awP4kGkuj","metadata":{"id":"Ur7awP4kGkuj"},"outputs":[],"source":["# Create a PCA object\n","pca = PCA()\n","\n","# Fit the PCA model on the training data\n","pca.fit(x_train)\n","\n","# Compute the cumulative sum of explained variance\n","var_exp = np.cumsum(pca.explained_variance_ratio_)\n","\n","# Create a bar plot of explained variance vs. number of components\n","plt.figure(figsize=(8, 6))\n","plt.bar(range(1, len(var_exp) + 1), var_exp, align='center', label='Individual explained variance')\n","plt.step(range(1, len(var_exp) + 1), var_exp, where='mid', label='Cumulative explained variance')\n","plt.ylabel('Explained variance ratio')\n","plt.xlabel('Principal components')\n","plt.legend(loc='best')\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"QVQGxrk1Ylfc","metadata":{"id":"QVQGxrk1Ylfc"},"outputs":[],"source":["# Validation Split\n","x_train2, x_valid, y_train2, y_valid = train_test_split(x_train, y_train, test_size=0.1)\n","\n","# Define the model\n","model2 = tf.keras.Sequential([\n","    tf.keras.layers.Dense(12, activation='relu', input_shape=(x_train.shape[1],), kernel_initializer=RandomUniform),\n","    tf.keras.layers.Dense(y_train.shape[1], activation='softmax')\n","])\n","\n","# Compile the model\n","model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","start_time = time.time()\n","\n","history2 = model2.fit(x_train2, y_train2, batch_size=152, epochs=225, verbose=0, validation_data=(x_valid,y_valid))\n","\n","end_time = time.time()\n","\n","time_per_epoch2 = (end_time - start_time) / len(history2.history['loss'])\n","\n","# Calculate average cross-entropy loss for train and test sets\n","train_preds2 = model2.predict(x_train)\n","train_avCRE2 = log_loss(y_train, train_preds2) / len(x_train)\n","\n","test_preds2 = model2.predict(x_test)\n","test_avCRE2 = log_loss(y_test, test_preds2) / len(x_test)\n","\n","# Extract the training and validation losses for each epoch\n","train_losses_per_epoch2 = history2.history['loss']\n","test_losses_per_epoch2 = history2.history['val_loss']\n","\n","# Calculate the average cross-entropy loss per epoch for train and test sets\n","train_avCRE_per_epoch2 = [loss / len(x_train) for loss in train_losses_per_epoch2]\n","test_avCRE_per_epoch2 = [loss / len(x_test) for loss in test_losses_per_epoch2]\n","\n","print(\"Average Cross Entropy Loss for Train Set:\", train_avCRE2)\n","print(\"Average Cross Entropy Loss for Test Set:\", test_avCRE2)\n","\n","print(\"Average Cross Entropy Loss per Epoch for Train Set:\", train_avCRE_per_epoch2)\n","print(\"Average Cross Entropy Loss per Epoch for Test Set:\", test_avCRE_per_epoch2)\n","\n","print(\"Computing time per epoch: \", time_per_epoch2)\n"]},{"cell_type":"code","execution_count":null,"id":"OTP_2Jhlae_C","metadata":{"id":"OTP_2Jhlae_C"},"outputs":[],"source":["# Plot the loss per epoch\n","plt.plot(train_losses_per_epoch2, label='Train')\n","plt.plot(test_losses_per_epoch2, label='Test')\n","plt.title('Cross-Entropy Loss per Epoch, h_low=12')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","# Evaluate the model on the test set\n","test_loss2, test_accuracy2  = model2.evaluate(x_test, y_test, verbose=0)\n","\n","print(\"Test Accuracy: {:.2f}%\".format(test_accuracy2 * 100))\n","\n","# Need to work out why this model has over 100% accuracy. Will come back to it.\n","\n","# There is a higher loss than the first model, but not by much.\n"]},{"cell_type":"code","execution_count":null,"id":"L0xCjgh9f3pG","metadata":{"id":"L0xCjgh9f3pG"},"outputs":[],"source":["test_pred_labels2 = np.argmax(test_preds2, axis=1)\n","\n","# Compute the confusion matrix\n","conf_matrix2 = confusion_matrix(test_pred_labels2,y_test_labels)\n","\n","# Normalize the confusion matrix\n","normalized_conf_matrix2 = conf_matrix2.astype('float') / conf_matrix2.sum(axis=1)[:, np.newaxis]\n","\n","# Convert to percentage\n","percentage_conf_matrix2 = normalized_conf_matrix2 * 100\n","\n","print(\"Confusion Matrix (Percentage):\")\n","print(\"Confusion Matrix (Percentage):\")\n","for row in percentage_conf_matrix2:\n","    formatted_row = [\"{:.2f}\".format(x) for x in row]\n","    print(\" \".join(formatted_row))\n"]},{"cell_type":"markdown","id":"_RwWCDRuAglf","metadata":{"id":"_RwWCDRuAglf"},"source":["# **Part 4**"]},{"cell_type":"code","execution_count":null,"id":"4j3X8kIwAlcK","metadata":{"id":"4j3X8kIwAlcK"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# Define the variance threshold\n","var_threshold = 0.9\n","\n","# Loop through each dataframe and perform PCA on numeric columns\n","for i, df in enumerate([e1, e2, e3, e4, e5, e6]):\n","    numeric_df = df.select_dtypes(include=['float64', 'int64'])\n","    pca = PCA(n_components=var_threshold)\n","    pca.fit(numeric_df)\n","    n_components = pca.n_components_\n","    print(f\"Number of features that capture 90% of the variance for e{i+1}: {n_components}\")\n","\n","# Our variance is highly concentrated to only a few features for every class.\n","# h1 + h2 ... = 13, which is the same dimension used for h_low.\n","# Going to use h=p where p is the total number of features for h_high\n"]},{"cell_type":"code","execution_count":null,"id":"xxerGxciHakV","metadata":{"id":"xxerGxciHakV"},"outputs":[],"source":["# Define the variance threshold\n","var_threshold = 0.9\n","\n","# Define the number of principal components to try\n","max_components = 10\n","\n","# Loop through each dataframe and perform PCA on numeric columns\n","for i, df in enumerate([e1, e2, e3, e4, e5, e6]):\n","    numeric_df = df.select_dtypes(include=['float64', 'int64'])\n","    pca = PCA(n_components=max_components)\n","    pca.fit(numeric_df)\n","    var_ratio = pca.explained_variance_ratio_\n","    cum_var_ratio = np.cumsum(var_ratio)\n","    n_components = np.argmax(cum_var_ratio >= var_threshold) + 1\n","    plt.plot(np.arange(1, max_components+1), cum_var_ratio, label=f'e{i+1}')\n","    \n","plt.xlabel('Number of principal components')\n","plt.ylabel('Percentage of explained variance')\n","plt.axhline(y=var_threshold, color='black', linestyle='--', label=f'{var_threshold:.0%} threshold')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"CS_DoEzsD30m","metadata":{"id":"CS_DoEzsD30m"},"outputs":[],"source":["# Validation Split\n","x_train3, x_valid, y_train3, y_valid = train_test_split(x_train, y_train, test_size=0.1)\n","\n","# Define the model\n","model3 = tf.keras.Sequential([\n","    tf.keras.layers.Dense(13, activation='relu', input_shape=(x_train.shape[1],), kernel_initializer=RandomUniform),\n","    tf.keras.layers.Dense(y_train.shape[1], activation='softmax')\n","])\n","\n","# Compile the model\n","model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","start_time = time.time()\n","\n","history3 = model3.fit(x_train3, y_train3, batch_size=152, epochs=160, verbose=0, validation_data=(x_valid,y_valid))\n","\n","end_time = time.time()\n","\n","time_per_epoch3 = (end_time - start_time) / len(history3.history['loss'])\n","\n","# Calculate average cross-entropy loss for train and test sets\n","train_preds3 = model3.predict(x_train)\n","train_avCRE3 = log_loss(y_train, train_preds3) / len(x_train)\n","\n","test_preds3 = model3.predict(x_test)\n","test_avCRE3 = log_loss(y_test, test_preds3) / len(x_test)\n","\n","# Extract the training and validation losses for each epoch\n","train_losses_per_epoch3 = history3.history['loss']\n","test_losses_per_epoch3 = history3.history['val_loss']\n","\n","# Calculate the average cross-entropy loss per epoch for train and test sets\n","train_avCRE_per_epoch3 = [loss / len(x_train) for loss in train_losses_per_epoch3]\n","test_avCRE_per_epoch3 = [loss / len(x_test) for loss in test_losses_per_epoch3]\n","\n","print(\"Average Cross Entropy Loss for Train Set:\", train_avCRE3)\n","print(\"Average Cross Entropy Loss for Test Set:\", test_avCRE3)\n","\n","print(\"Average Cross Entropy Loss per Epoch for Train Set:\", train_avCRE_per_epoch3)\n","print(\"Average Cross Entropy Loss per Epoch for Test Set:\", test_avCRE_per_epoch3)\n","\n","print(\"Computing time per epoch: \", time_per_epoch3)\n"]},{"cell_type":"code","execution_count":null,"id":"s9V-b63mGUNt","metadata":{"id":"s9V-b63mGUNt"},"outputs":[],"source":["# Plot the loss per epoch\n","plt.plot(train_losses_per_epoch3, label='Train')\n","plt.plot(test_losses_per_epoch3, label='Test')\n","plt.title('Cross-Entropy Loss per Epoch,  h_3=13')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","# Evaluate the model on the test set\n","test_loss3, test_accuracy3  = model3.evaluate(x_test, y_test, verbose=0)\n","\n","print(\"Test Accuracy: {:.2f}%\".format(test_accuracy3 * 100))\n"]},{"cell_type":"code","execution_count":null,"id":"tmsi_de5GVbB","metadata":{"id":"tmsi_de5GVbB"},"outputs":[],"source":["test_pred_labels3 = np.argmax(test_preds3, axis=1)\n","\n","# Compute the confusion matrix\n","conf_matrix3 = confusion_matrix(test_pred_labels3,y_test_labels)\n","\n","# Normalize the confusion matrix\n","normalized_conf_matrix3 = conf_matrix3.astype('float') / conf_matrix3.sum(axis=1)[:, np.newaxis]\n","\n","# Convert to percentage\n","percentage_conf_matrix3 = normalized_conf_matrix3 * 100\n","\n","print(\"Confusion Matrix (Percentage):\")\n","print(\"Confusion Matrix (Percentage):\")\n","for row in percentage_conf_matrix3:\n","    formatted_row = [\"{:.2f}\".format(x) for x in row]\n","    print(\" \".join(formatted_row))\n"]},{"cell_type":"code","execution_count":null,"id":"aFPe0m-LHbHD","metadata":{"id":"aFPe0m-LHbHD"},"outputs":[],"source":["# Validation Split\n","x_train4, x_valid, y_train4, y_valid = train_test_split(x_train, y_train, test_size=0.1)\n","\n","# Define the model\n","model4 = tf.keras.Sequential([\n","    tf.keras.layers.Dense(81, activation='relu', input_shape=(x_train.shape[1],), kernel_initializer=RandomUniform),\n","    tf.keras.layers.Dense(y_train.shape[1], activation='softmax')\n","])\n","\n","# Compile the model\n","model4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","start_time = time.time()\n","\n","history4 = model4.fit(x_train4, y_train4, batch_size=152, epochs=160, verbose=0, validation_data=(x_valid,y_valid))\n","\n","end_time = time.time()\n","\n","time_per_epoch4 = (end_time - start_time) / len(history4.history['loss'])\n","\n","# Calculate average cross-entropy loss for train and test sets\n","train_preds4 = model4.predict(x_train)\n","train_avCRE4 = log_loss(y_train, train_preds4) / len(x_train)\n","\n","test_preds4 = model4.predict(x_test)\n","test_avCRE4 = log_loss(y_test, test_preds4) / len(x_test)\n","\n","# Extract the training and validation losses for each epoch\n","train_losses_per_epoch4 = history4.history['loss']\n","test_losses_per_epoch4 = history4.history['val_loss']\n","\n","# Calculate the average cross-entropy loss per epoch for train and test sets\n","train_avCRE_per_epoch4 = [loss / len(x_train) for loss in train_losses_per_epoch4]\n","test_avCRE_per_epoch4 = [loss / len(x_test) for loss in test_losses_per_epoch4]\n","\n","print(\"Average Cross Entropy Loss for Train Set:\", train_avCRE4)\n","print(\"Average Cross Entropy Loss for Test Set:\", test_avCRE4)\n","\n","print(\"Average Cross Entropy Loss per Epoch for Train Set:\", train_avCRE_per_epoch4)\n","print(\"Average Cross Entropy Loss per Epoch for Test Set:\", test_avCRE_per_epoch4)\n","\n","print(\"Computing time per epoch: \", time_per_epoch4)\n"]},{"cell_type":"code","execution_count":null,"id":"b8a4928b","metadata":{"id":"b8a4928b"},"outputs":[],"source":["# Plot the loss per epoch\n","plt.plot(train_losses_per_epoch4, label='Train')\n","plt.plot(test_losses_per_epoch4, label='Test')\n","plt.title('Cross-Entropy Loss per Epoch,  h_high=81')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","# Evaluate the model on the test set\n","test_loss4, test_accuracy4  = model4.evaluate(x_test, y_test, verbose=0)\n","\n","print(\"Test Accuracy: {:.2f}%\".format(test_accuracy4 * 100))\n"]},{"cell_type":"code","execution_count":null,"id":"77c221da","metadata":{"id":"77c221da"},"outputs":[],"source":["test_pred_labels4 = np.argmax(test_preds4, axis=1)\n","\n","# Compute the confusion matrix\n","conf_matrix4 = confusion_matrix(test_pred_labels4,y_test_labels)\n","\n","# Normalize the confusion matrix\n","normalized_conf_matrix4 = conf_matrix4.astype('float') / conf_matrix4.sum(axis=1)[:, np.newaxis]\n","\n","# Convert to percentage\n","percentage_conf_matrix4 = normalized_conf_matrix4 * 100\n","\n","print(\"Confusion Matrix (Percentage):\")\n","print(\"Confusion Matrix (Percentage):\")\n","for row in percentage_conf_matrix4:\n","    formatted_row = [\"{:.2f}\".format(x) for x in row]\n","    print(\" \".join(formatted_row))\n"]},{"cell_type":"markdown","id":"Yv-cJvpvbziu","metadata":{"id":"Yv-cJvpvbziu"},"source":["# **Part 5**"]},{"cell_type":"code","execution_count":null,"id":"fLNl0L90b4fS","metadata":{"id":"fLNl0L90b4fS"},"outputs":[],"source":["# Define the model, this time with seperate objects for each layer\n","input_layer = tf.keras.Input(shape=(x_train.shape[1],))\n","hidden_layer = tf.keras.layers.Dense(81, activation='sigmoid', kernel_initializer=RandomUniform)(input_layer)\n","output_layer = tf.keras.layers.Dense(y_train.shape[1], activation='softmax')(hidden_layer)\n","\n","mlp_high = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n","\n","# Compile the model\n","mlp_high.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","history = mlp_high.fit(x_train, y_train, batch_size=42, epochs=150, verbose=0, validation_data=(x_test,y_test))\n"]},{"cell_type":"code","execution_count":null,"id":"y5rvbhQZcC5J","metadata":{"id":"y5rvbhQZcC5J"},"outputs":[],"source":["# Create new model to study the hidden layer\n","hidden_layer_model = tf.keras.Model(inputs=mlp_high.input, outputs=mlp_high.layers[1].output)\n","\n","# Compute Zn for each case in x_train\n","Zn_train = hidden_layer_model.predict(x_train)\n","Zn_test = hidden_layer_model.predict(x_test)\n"]},{"cell_type":"code","execution_count":null,"id":"C6gPya3zck0b","metadata":{"id":"C6gPya3zck0b"},"outputs":[],"source":["latent_dim = 81\n","input_dim = 81\n","sparsity_target = 0.1\n","sparsity_weight = 1e-3\n","\n","class Autoencoder(tf.keras.Model):\n","    def __init__(self, latent_dim, input_dim):\n","        super(Autoencoder, self).__init__()\n","\n","        self.latent_dim = latent_dim   \n","        self.encoder = tf.keras.Sequential([\n","            tf.keras.layers.Input(shape=(input_dim,)),\n","            tf.keras.layers.Dense(latent_dim, activation='sigmoid'),\n","        ])\n","\n","        self.decoder = tf.keras.Sequential([\n","            tf.keras.layers.Dense(input_dim, activation='sigmoid'),\n","        ])\n","\n","    def call(self, x):\n","        encoded = self.encoder(x)\n","        decoded = self.decoder(encoded)\n","        return decoded\n","\n","autoencoder = Autoencoder(latent_dim, input_dim)\n","\n","# Define RMSE as a custom metric\n","def rmse(y_true, y_pred):\n","    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n","\n","# Define the KL divergence sparsity penalty\n","def kl_divergence(p, p_hat):\n","    return p * tf.math.log(p / p_hat) + (1 - p) * tf.math.log((1 - p) / (1 - p_hat))\n","\n","# Define the sparse loss function\n","def sparse_loss(y_true, y_pred):\n","    mse_loss = tf.reduce_mean(tf.square(y_pred - y_true))\n","    hidden_layer = autoencoder.encoder(y_true)\n","    p_hat = tf.reduce_mean(hidden_layer, axis=0)\n","    sparsity_loss = tf.reduce_sum(kl_divergence(sparsity_target, p_hat))\n","    total_loss = mse_loss + sparsity_weight * sparsity_loss\n","    return total_loss\n","\n","autoencoder.compile(optimizer='adam', loss=sparse_loss, metrics=[rmse])\n","\n","history_aec = autoencoder.fit(Zn_train, Zn_train,\n","                              epochs=75,\n","                              shuffle=True,\n","                              validation_data=(Zn_test, Zn_test),\n","                              verbose=1)\n"]},{"cell_type":"code","execution_count":null,"id":"lioTfphUcoLL","metadata":{"id":"lioTfphUcoLL"},"outputs":[],"source":["train_rmse = history_aec.history['rmse']\n","val_rmse = history_aec.history['val_rmse']\n","epochs = range(1, len(train_rmse) + 1)\n","\n","plt.plot(epochs, train_rmse, 'b', label='Training RMSE')\n","plt.plot(epochs, val_rmse, 'r', label='Test RMSE')\n","plt.xlabel('Epochs')\n","plt.ylabel('RMSE')\n","plt.legend()\n","plt.title('RMSE vs. Epochs')\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"FXr-culLcu3T","metadata":{"id":"FXr-culLcu3T"},"outputs":[],"source":["# Copy and paste model3 from other file\n","\n","# Define the model\n","model3 = tf.keras.Sequential([\n","    tf.keras.layers.Dense(81, activation='relu', input_shape=(x_train.shape[1],), kernel_initializer=RandomUniform),\n","    tf.keras.layers.Dense(y_train.shape[1], activation='softmax')\n","])\n","\n","# Compile the model\n","model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","history3 = model3.fit(x_train, y_train, batch_size=42, epochs=150, verbose=0, validation_data=(x_test,y_test))\n"]},{"cell_type":"code","execution_count":null,"id":"dIu_Q-Adc0su","metadata":{"id":"dIu_Q-Adc0su"},"outputs":[],"source":["# Extract weights and biases from the first dense layer of model3 and the encoder\n","model3_dense1_weights, model3_dense1_biases = model3.layers[0].get_weights()\n","\n","encoder_weights, encoder_biases = autoencoder.encoder.layers[0].get_weights()\n","\n","# Verify that the weights and biases have the same dimensions\n","print(\"Model3 Weights Size: \", model3_dense1_weights.shape)\n","print(\"Model3 Biases Size: \", model3_dense1_biases.shape)\n","print(\"Encoder Weights Size: \", encoder_weights.shape)\n","print(\"Encoder Biases Size: \", encoder_biases.shape)\n"]},{"cell_type":"code","execution_count":null,"id":"shZozm6cc2-b","metadata":{"id":"shZozm6cc2-b"},"outputs":[],"source":["Zn_combined = np.concatenate((Zn_train, Zn_test), axis=0)\n","\n","# Define the modified MLP_12 model\n","MLP_12 = tf.keras.Sequential([\n","    model3.layers[0],\n","    autoencoder.encoder,\n","    tf.keras.layers.Dense(81, activation='linear')  # Change the output dimension to 82\n","])\n","\n","# Compile the modified model\n","MLP_12.compile(optimizer='adam', loss='mse')\n","\n","# Train the modified model\n","history_MLP_12 = MLP_12.fit(x_train, Zn_train,\n","                            batch_size=42,\n","                            epochs=150,\n","                            verbose=0,\n","                            validation_data=(x_test, Zn_test))\n","\n","# Combine x_train and x_test\n","x_combined = np.concatenate((x_train, x_test), axis=0)\n","y_combined = np.concatenate((y_train, y_test), axis=0)\n","\n","# Predict the output using the modified MLP_12 model\n","U_n = MLP_12.predict(x_combined)\n"]},{"cell_type":"code","execution_count":null,"id":"0MrTcSnVdDCv","metadata":{"id":"0MrTcSnVdDCv"},"outputs":[],"source":["# Split the U_n dataset into training and test sets (80/20)\n","U_n_train, U_n_test, y_train_new, y_test_new = train_test_split(U_n, y_combined, test_size=0.2, random_state=42)\n","U_n.shape\n"]},{"cell_type":"code","execution_count":null,"id":"AntGRz02dFxx","metadata":{"id":"AntGRz02dFxx"},"outputs":[],"source":["# Define the MLP3 model\n","MLP3 = tf.keras.Sequential([\n","    tf.keras.layers.Input(shape=(U_n_train.shape[1],)),\n","    tf.keras.layers.Dense(40, activation='relu'),\n","    tf.keras.layers.Dense(y_train.shape[1], activation='softmax')\n","])\n","\n","# Compile the MLP3 model\n","MLP3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the MLP3 model\n","history_MLP3 = MLP3.fit(U_n_train, y_train_new,\n","                        batch_size=42,\n","                        epochs=250,\n","                        verbose=0,\n","                        validation_data=(U_n_test, y_test_new))\n","\n","train_preds =  MLP3.predict(U_n_train)\n","train_avCRE = log_loss(y_train_new, train_preds) / len(U_n_train)\n","\n","test_preds = MLP3.predict(U_n_test)\n","test_avCRE = log_loss(y_test_new, test_preds) / len(U_n_test)\n","\n","# Extract the training and validation losses for each epoch\n","train_losses_per_epoch = history_MLP3.history['loss']\n","test_losses_per_epoch = history_MLP3.history['val_loss']\n","\n","# Calculate the average cross-entropy loss per epoch for train and test sets\n","train_avCRE_per_epoch = [loss / len(U_n_train) for loss in train_losses_per_epoch]\n","test_avCRE_per_epoch = [loss / len(U_n_test) for loss in test_losses_per_epoch]\n","\n","print(\"Average Cross Entropy Loss for Train Set:\", train_avCRE)\n","print(\"Average Cross Entropy Loss for Test Set:\", test_avCRE)\n","\n","print(\"Average Cross Entropy Loss per Epoch for Train Set:\", train_avCRE_per_epoch)\n","print(\"Average Cross Entropy Loss per Epoch for Test Set:\", test_avCRE_per_epoch)\n"]},{"cell_type":"code","execution_count":null,"id":"cH2j_FiMdUVU","metadata":{"id":"cH2j_FiMdUVU"},"outputs":[],"source":["plt.plot(train_avCRE_per_epoch, label='Train')\n","plt.plot(test_avCRE_per_epoch, label='Test')\n","plt.title('Average Cross-Entropy Loss per Epoch, MLP3')\n","plt.xlabel('Epoch')\n","plt.ylabel('Average Loss')\n","plt.legend()\n","plt.show()\n","\n","# Evaluate the MLP3 model on the U_n_test set\n","test_loss_MLP3, test_accuracy_MLP3 = MLP3.evaluate(U_n_test, y_test_new, verbose=0)\n","\n","print(\"Test Accuracy: {:.2f}%\".format(test_accuracy_MLP3 * 100))\n"]},{"cell_type":"code","execution_count":null,"id":"mXK0flHaiJjX","metadata":{"id":"mXK0flHaiJjX"},"outputs":[],"source":["# Create and print the confusion matrix.\n","test_pred_labels = np.argmax(test_preds, axis=1)\n","y_test_array = y_test_new\n","y_test_labels = np.argmax(y_test_array, axis=1)\n","\n","# Compute the confusion matrix\n","conf_matrix = confusion_matrix(test_pred_labels, y_test_labels)\n","\n","# Normalize the confusion matrix\n","normalized_conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n","\n","# Convert to percentage\n","percentage_conf_matrix = normalized_conf_matrix * 100\n","\n","print(\"Confusion Matrix (Percentage):\")\n","print(\"Confusion Matrix (Percentage):\")\n","for row in percentage_conf_matrix:\n","    formatted_row = [\"{:.2f}\".format(x) for x in row]\n","    print(\" \".join(formatted_row))"]},{"cell_type":"code","execution_count":null,"id":"EMVxhp5OdWgn","metadata":{"id":"EMVxhp5OdWgn"},"outputs":[],"source":["MLP_long = tf.keras.Sequential([\n","    MLP_12.layers[0],  # First hidden layer from MLP_12\n","    MLP_12.layers[1],  # Second hidden layer (output layer of MLP_12)\n","    MLP3.layers[0],  # \n","    MLP3.layers[1],  # Third hidden layer from MLP3\n","])\n","\n","\n","# Compile the MLP_long model\n","MLP_long.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","# Train the MLP_long model\n","history_MLP_long = MLP_long.fit(x_train, y_train,\n","                                batch_size=42,\n","                                epochs=50,\n","                                verbose=0,\n","                                validation_data=(x_test, y_test))\n","\n","train_preds_long =  MLP_long.predict(x_train)\n","train_avCRE_long = log_loss(y_train, train_preds_long) / len(x_train)\n","\n","test_preds_long = MLP_long.predict(x_test)\n","test_avCRE_long = log_loss(y_test, test_preds_long) / len(x_test)\n","\n","# Extract the training and validation losses for each epoch\n","train_losses_per_epoch_long = history_MLP_long.history['loss']\n","test_losses_per_epoch_long = history_MLP_long.history['val_loss']\n","\n","# Calculate the average cross-entropy loss per epoch for train and test sets\n","train_avCRE_per_epoch_long = [loss / len(x_train) for loss in train_losses_per_epoch_long]\n","test_avCRE_per_epoch_long = [loss / len(x_test) for loss in test_losses_per_epoch_long]\n","\n","print(\"Average Cross Entropy Loss for Train Set:\", train_avCRE_long)\n","print(\"Average Cross Entropy Loss for Test Set:\", test_avCRE_long)\n","\n","print(\"Average Cross Entropy Loss per Epoch for Train Set:\", train_avCRE_per_epoch_long)\n","print(\"Average Cross Entropy Loss per Epoch for Test Set:\", test_avCRE_per_epoch_long)\n"]},{"cell_type":"code","execution_count":null,"id":"SgnMlsfndcTT","metadata":{"id":"SgnMlsfndcTT"},"outputs":[],"source":["plt.plot(train_avCRE_per_epoch_long, label='Train')\n","plt.plot(test_avCRE_per_epoch_long, label='Test')\n","plt.title('Average Cross-Entropy Loss per Epoch, MLP_long')\n","plt.xlabel('Epoch')\n","plt.ylabel('Average Loss')\n","plt.legend()\n","plt.show()\n","\n","# Evaluate the MLP3 model on the U_n_test set\n","test_loss_long, test_accuracy_long = MLP_long.evaluate(x_test, y_test, verbose=0)\n","\n","print(\"Test Accuracy: {:.2f}%\".format(test_accuracy_long * 100))\n"]},{"cell_type":"markdown","id":"UpoNWYvvJkij","metadata":{"id":"UpoNWYvvJkij"},"source":[]},{"cell_type":"code","execution_count":null,"id":"cd8aeb21","metadata":{"id":"cd8aeb21"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d4fbbddd","metadata":{"id":"d4fbbddd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"57cafaf4","metadata":{"id":"57cafaf4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"8c8a4215","metadata":{"id":"8c8a4215"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d8710dc1","metadata":{"id":"d8710dc1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"af24b9e9","metadata":{"id":"af24b9e9"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"907f3a3c","metadata":{"id":"907f3a3c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"3b4dd8cc","metadata":{"id":"3b4dd8cc"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"c65d6319","metadata":{"id":"c65d6319"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d99e01d7","metadata":{"id":"d99e01d7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"4afb4a9c","metadata":{"id":"4afb4a9c"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["RErzeF8QJAEY","J83M9xTZJWCZ","OT33y1lXhPfG","vX3J63TkhFWi","_RwWCDRuAglf","Yv-cJvpvbziu"],"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"oldHeight":823.85,"position":{"height":"908.85px","left":"6px","right":"20px","top":"44px","width":"371px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"varInspector_section_display":"block","window_display":true}},"nbformat":4,"nbformat_minor":5}